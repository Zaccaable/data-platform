{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50143c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=knmi daily weather to bronze, master=local[*]) created by __init__ at /tmp/ipykernel_44/826370988.py:23 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44/826370988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.hadoop.fs.s3a.aws.credentials.provider'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=knmi daily weather to bronze, master=local[*]) created by __init__ at /tmp/ipykernel_44/826370988.py:23 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"knmi daily weather to bronze\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://192.168.86.192:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.getenv('MINIO_ROOT_USER'))\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.getenv('MINIO_ROOT_PASSWORD'))\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.driver.memory\", \"8g\")\n",
    "    .set(\"spark.executor.memory\", \"8g\")\n",
    "    .set(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog.warehouse\", \"s3a://warehouse\") \n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    ")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession(sc).builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93452bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load('s3a://bronze-knmi/daggegevens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f542192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df.write.option('path', 's3a://test-bucket/testtable').saveAsTable('test_table2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce164e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|weather_station_code|      date|\n",
      "+--------------------+----------+\n",
      "|                 286|1999-11-18|\n",
      "|                 331|1999-11-18|\n",
      "|                 235|1999-11-18|\n",
      "|                 240|1999-11-18|\n",
      "|                 380|1999-11-18|\n",
      "|                 210|1999-11-18|\n",
      "|                 277|1999-11-18|\n",
      "|                 275|1999-11-18|\n",
      "|                 323|1999-11-18|\n",
      "|                 344|1999-11-18|\n",
      "|                 285|1999-11-18|\n",
      "|                 330|1999-11-18|\n",
      "|                 348|1999-11-18|\n",
      "|                 265|1999-11-18|\n",
      "|                 251|1999-11-18|\n",
      "|                 283|1999-11-18|\n",
      "|                 267|1999-11-18|\n",
      "|                 225|1999-11-18|\n",
      "|                 324|1999-11-18|\n",
      "|                 278|1999-11-18|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.weather_station_code, df.date).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b1d7a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44/2462691895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc96376d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE DATABASE jdp\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99cd1c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''USE jdp''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25b948ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS test2 (id int, name string)  USING delta LOCATION 's3a://warehouse/'\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f1d84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''INSERT INTO test VALUES (1, 'my first insert')''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d5875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|           name|\n",
      "+---+---------------+\n",
      "|  1|my first insert|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT * FROM spark_catalog.default.test''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ff13fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+\n",
      "|version|          timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+\n",
      "|      0|2022-12-20 20:53:43|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|          null|         true|              {}|        null|\n",
      "+-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''DESCRIBE HISTORY test2''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcd317f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'catalog'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_634/2072385374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistDatabases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'catalog'"
     ]
    }
   ],
   "source": [
    "spark.sql.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b63e45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load('s3a://bronze-knmi/daggegevens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7aa31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('knmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0be3e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT YYYYMMDD)|\n",
      "+------------------------+\n",
      "|                   44548|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''select count(DISTINCT YYYYMMDD) from knmi''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28032e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''create table if not exists bronze_daggegevens USING delta LOCATION 's3a://bronze-knmi/daggegevens'  ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7ed6471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      1|2022-12-20 21:22:19|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          0|          null|        false|{numFiles -> 4, n...|        null|\n",
      "|      0|2021-08-31 21:07:13|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 16, ...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''DESCRIBE HISTORY bronze_daggegevens''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "04640986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      9|2022-12-22 21:26:53|  null|    null|    MERGE|{predicate -> ((k...|null|    null|     null|          8|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      8|2022-12-21 21:06:10|  null|    null|    MERGE|{predicate -> ((k...|null|    null|     null|          7|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      7|2022-12-20 22:05:04|  null|    null|    MERGE|{predicate -> ((k...|null|    null|     null|          6|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      6|2022-12-20 22:02:21|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          5|          null|        false|{numFiles -> 4, n...|        null|\n",
      "|      5|2022-12-20 22:00:59|  null|    null|    MERGE|{predicate -> ((k...|null|    null|     null|          4|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      4|2022-12-20 21:59:14|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          3|          null|        false|{numFiles -> 4, n...|        null|\n",
      "|      3|2022-12-20 21:47:10|  null|    null|    MERGE|{predicate -> ((k...|null|    null|     null|          2|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      2|2022-12-20 21:39:15|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 4, n...|        null|\n",
      "|      1|2022-12-20 21:22:19|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          0|          null|        false|{numFiles -> 4, n...|        null|\n",
      "|      0|2021-08-31 21:07:13|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 16, ...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://bronze-knmi/daggegevens\")\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58638a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849211\n"
     ]
    }
   ],
   "source": [
    "print(spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"s3a://bronze-knmi/daggegevens\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ea141bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23500\n"
     ]
    }
   ],
   "source": [
    "print(spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"s3a://bronze-knmi/daggegevens\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63fc9e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849211\n"
     ]
    }
   ],
   "source": [
    "print(spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"s3a://bronze-knmi/daggegevens\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12505acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895177\n"
     ]
    }
   ],
   "source": [
    "print(spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"s3a://bronze-knmi/daggegevens\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c01087ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://bronze-knmi/daggegevens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2193e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('dag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f94f476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "|STN|YYYYMMDD|load_datetime|\n",
      "+---+--------+-------------+\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "select STN, YYYYMMDD, load_datetime from dag where YYYYMMDD = '20220101' order by STN\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7144d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"s3a://bronze-knmi/daggegevens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedd3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://bronze-knmi/daggegevens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573ff85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|       load_datetime|YYYYMMDD|\n",
      "+--------------------+--------+\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "|2022-12-22 21:26:...|20221221|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df.select('load_datetime', 'YYYYMMDD').orderBy(desc('load_datetime')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "abe481d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdw_raw_df = spark.read.csv('s3a://test-bucket/rdw/', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1644484f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 206:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|kenteken|count|\n",
      "+--------+-----+\n",
      "|  00LHG1|    2|\n",
      "|  00TSB9|    2|\n",
      "|  00VXS7|    2|\n",
      "|  01JDPS|    2|\n",
      "|  01LTRH|    2|\n",
      "|  01NTN4|    2|\n",
      "|  01SFB4|    2|\n",
      "|  00LDF3|    2|\n",
      "|  01JPH4|    2|\n",
      "|  02LLHX|    2|\n",
      "|  02NXK2|    2|\n",
      "|  02XFT6|    2|\n",
      "|  02XKK2|    2|\n",
      "|  02XZPN|    2|\n",
      "|  02YB99|    2|\n",
      "|  03GHTL|    2|\n",
      "|  03JTG1|    2|\n",
      "|  03SXB1|    2|\n",
      "|  03TJH9|    2|\n",
      "|  03XPH8|    2|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdw_raw_df.groupBy('kenteken').count().filter('count > 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "845cf572",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdw_df =  rdw_raw_df.select('kenteken', col('Brandstof volgnummer').alias('brandstof_volgnummer'), col('Brandstof omschrijving').alias('brandstof_omschrijving'), col('Klasse hybride elektrisch voertuig').alias('klasse_hybride'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "61e2f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdw_df.coalesce(1).write.format('parquet').mode('overwrite').save('s3a://test-bucket/rdw_brandstof/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "853d684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "226ae2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdw_df.createOrReplaceTempView('rdw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fc5a23c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 228:================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------------------------------+\n",
      "|kenteken|count(1)|collect_list(brandstof_omschrijving)|\n",
      "+--------+--------+------------------------------------+\n",
      "|  00LDF3|       2|                [Benzine, Elektri...|\n",
      "|  00LHG1|       2|                [Benzine, Elektri...|\n",
      "|  00TSB9|       2|                [Benzine, Elektri...|\n",
      "|  00VXS7|       2|                      [LPG, Benzine]|\n",
      "|  01JDPS|       2|                      [Benzine, LPG]|\n",
      "|  01JPH4|       2|                [Benzine, Elektri...|\n",
      "|  01LTRH|       2|                      [Benzine, LPG]|\n",
      "|  01NTN4|       2|                      [LPG, Benzine]|\n",
      "|  01SFB4|       2|                [Benzine, Elektri...|\n",
      "|  02LLHX|       2|                      [Benzine, LPG]|\n",
      "|  02NXK2|       2|                [Benzine, Elektri...|\n",
      "|  02XFT6|       2|                [Benzine, Elektri...|\n",
      "|  02XKK2|       2|                [Benzine, Elektri...|\n",
      "|  02XZPN|       2|                      [LPG, Benzine]|\n",
      "|  02YB99|       2|                      [Benzine, LPG]|\n",
      "|  03GHTL|       2|                      [LPG, Benzine]|\n",
      "|  03JTG1|       2|                [Benzine, Elektri...|\n",
      "|  03SXB1|       2|                [Benzine, Elektri...|\n",
      "|  03TJH9|       2|                      [LPG, Benzine]|\n",
      "|  03XPH8|       2|                [Benzine, Elektri...|\n",
      "+--------+--------+------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 228:===================================================>   (15 + 1) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "select kenteken, count(*), collect_list(brandstof_omschrijving)\n",
    "from rdw\n",
    "group by kenteken\n",
    "having count(*) > 1\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c189f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('dag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8cee7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==========>                                             (3 + 13) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|YYYYMMDD|\n",
      "+--------+\n",
      "|20230128|\n",
      "|20230127|\n",
      "|20230126|\n",
      "|20230125|\n",
      "|20230124|\n",
      "|20230123|\n",
      "|20230122|\n",
      "|20230121|\n",
      "|20230120|\n",
      "|20230119|\n",
      "|20230118|\n",
      "|20230117|\n",
      "|20230116|\n",
      "|20230115|\n",
      "|20230114|\n",
      "|20230113|\n",
      "|20221221|\n",
      "|20221220|\n",
      "|20221219|\n",
      "|20221218|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT distinct YYYYMMDD from dag order by YYYYMMDD desc\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f0d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load('s3a://silver-knmi/daggegevens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c0c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-29|      47|\n",
      "|2023-01-28|      47|\n",
      "|2023-01-27|      47|\n",
      "|2023-01-26|      47|\n",
      "|2023-01-25|      48|\n",
      "|2023-01-24|      48|\n",
      "|2023-01-23|      48|\n",
      "|2023-01-22|      47|\n",
      "|2023-01-21|      58|\n",
      "|2023-01-20|      47|\n",
      "|2023-01-19|      47|\n",
      "|2023-01-18|      48|\n",
      "|2023-01-17|      51|\n",
      "|2023-01-16|      47|\n",
      "|2023-01-15|      47|\n",
      "|2023-01-14|      61|\n",
      "|2023-01-13|      47|\n",
      "|2023-01-12|      47|\n",
      "|2023-01-11|      47|\n",
      "|2023-01-10|      47|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT date, count(*) from dag group by date order by 1 desc\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a636dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load(\"s3a://bronze-knmi/uurgegevens\").filter('YYYYMMDD is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baf73dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.drop('load_datetime').repartition(4).write.mode('overwrite').format('parquet').save(\"s3a://fontys/knmi/hourly_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff368e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('uur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cbbe841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+--------+\n",
      "|STN|YYYYMMDD|  H|count(1)|\n",
      "+---+--------+---+--------+\n",
      "+---+--------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select STN, YYYYMMDD, H, count(*)\n",
    "from uur\n",
    "group by STN, YYYYMMDD, H\n",
    "having count(*) > 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ad5640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet').load(\"s3a://fontys/knmi/hourly_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f46fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('hourly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "227798b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 90:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|left(YYYYMMDD, 4)|count(1)|\n",
      "+-----------------+--------+\n",
      "|             2023|   47472|\n",
      "|             2022|  402960|\n",
      "|             2021|  402960|\n",
      "|             2020|  412848|\n",
      "|             2019|  411720|\n",
      "|             2018|  411720|\n",
      "|             2017|  403392|\n",
      "|             2016|  412440|\n",
      "|             2015|  420480|\n",
      "|             2014|  416544|\n",
      "|             2013|  420480|\n",
      "|             2012|  421632|\n",
      "|             2011|  420480|\n",
      "|             2010|  411720|\n",
      "|             2009|  411720|\n",
      "|             2008|  418200|\n",
      "|             2007|  415248|\n",
      "|             2006|  419736|\n",
      "|             2005|  411696|\n",
      "|             2004|  412848|\n",
      "+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select left(YYYYMMDD, 4), count(*)\n",
    "from hourly\n",
    "group by left(YYYYMMDD, 4)\n",
    "order by 1 desc\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e812fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load('s3a://bronze-knmi/weather_stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "524f4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('load_datetime').dropDuplicates().repartition(1).write.mode('overwrite').format('parquet').save('s3a://fontys/knmi/weather_stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5a649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
