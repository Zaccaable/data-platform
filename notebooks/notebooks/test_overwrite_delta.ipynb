{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3312b3",
   "metadata": {},
   "source": [
    "# Initiate spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6b75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/31 07:09:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Spark minIO Test\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://192.168.86.192:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.getenv('MINIO_ROOT_USER'))\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.getenv('MINIO_ROOT_PASSWORD'))\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.driver.memory\", \"8g\")\n",
    "    .set(\"spark.executor.memory\", \"8g\")\n",
    "    .set(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    ")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "#sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc).builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c15d3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f21bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.821/08/31 07:10:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "DROP TABLE IF EXISTS test_bronze_knmi_daggegevens\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE test_bronze_knmi_daggegevens\n",
    "USING DELTA\n",
    "LOCATION 's3a://bronze-knmi/daggegevens'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e62783ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from_path = 's3a://bronze-knmi/daggegevens'\n",
    "to_path = 's3a://bronze-knmi/daggegevens_bu'\n",
    "numFiles = 16\n",
    "\n",
    "spark.read \\\n",
    " .format(\"delta\") \\\n",
    " .load(from_path) \\\n",
    " .repartition(numFiles) \\\n",
    " .write \\\n",
    " .format(\"delta\") \\\n",
    " .mode(\"overwrite\") \\\n",
    " .save(to_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5c535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from_path = 's3a://bronze-knmi/daggegevens'\n",
    "to_path = 's3a://bronze-knmi/daggegevens_bu'\n",
    "numFiles = 16\n",
    "\n",
    "spark.read \\\n",
    " .format(\"delta\") \\\n",
    " .load(to_path) \\\n",
    " .repartition(numFiles) \\\n",
    " .write \\\n",
    " .format(\"delta\") \\\n",
    " .mode(\"overwrite\") \\\n",
    " .save(from_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8873bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "DROP TABLE IF EXISTS test_bronze_knmi_daggegevens\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE test_bronze_knmi_daggegevens\n",
    "USING DELTA\n",
    "LOCATION 's3a://bronze-knmi/daggegevens'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM test_bronze_knmi_daggegevens\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b033d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "Select count(*) from test_bronze_knmi_daggegevens\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==>                                                 (469 + 16) / 10000]\r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "Vacuum test_bronze_knmi_daggegevens\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f6d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2006f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
