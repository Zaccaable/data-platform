{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3312b3",
   "metadata": {},
   "source": [
    "# Initiate spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6b75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/25 20:33:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/08/25 20:33:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, max, to_date\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Spark minIO Test\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://192.168.86.192:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.getenv('MINIO_ROOT_USER'))\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.getenv('MINIO_ROOT_PASSWORD'))\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.driver.memory\", \"8g\")\n",
    "    .set(\"spark.executor.memory\", \"8g\")\n",
    "    .set(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    ")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "#sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc).builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439ffd2",
   "metadata": {},
   "source": [
    "### Read data from the bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe41bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+--------+---------------+--------------------+\n",
      "|weather_station_code|longitude|latitude|altitude|weather_station|       load_datetime|\n",
      "+--------------------+---------+--------+--------+---------------+--------------------+\n",
      "|                 283|    6.657|  52.069|   29.10|         Hupsel|2021-08-24 19:47:...|\n",
      "|                 340|    4.342|  51.449|   19.20|    Woensdrecht|2021-08-24 19:47:...|\n",
      "|                 277|    6.200|  53.413|    2.90|     Lauwersoog|2021-08-24 19:47:...|\n",
      "|                 375|    5.707|  51.659|   22.00|         Volkel|2021-08-24 19:47:...|\n",
      "|                 312|    3.622|  51.768|    0.00|  Oosterschelde|2021-08-24 19:47:...|\n",
      "+--------------------+---------+--------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weather stations is a small dimension, so we will just regenerate is every time\n",
    "df = spark.read.format('delta').load('s3a://bronze-knmi/weather_stations')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bce63",
   "metadata": {},
   "source": [
    "### Transform and rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4e7823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+-----------+------------------+----------+----------+\n",
      "|weather_station_code| longitude|   latitude|   altitude|   weather_station|valid_from|  valid_to|\n",
      "+--------------------+----------+-----------+-----------+------------------+----------+----------+\n",
      "|                 209|4.51800000|52.46500000|       0E-8|            IJmond|2021-08-24|9999-12-31|\n",
      "|                 210|4.43000000|52.17100000|-0.20000000|     Valkenburg Zh|2021-08-24|9999-12-31|\n",
      "|                 215|4.43700000|52.14100000|-1.10000000|       Voorschoten|2021-08-24|9999-12-31|\n",
      "|                 225|4.55500000|52.46300000| 4.40000000|          IJmuiden|2021-08-24|9999-12-31|\n",
      "|                 235|4.78100000|52.92800000| 1.20000000|           De Kooy|2021-08-24|9999-12-31|\n",
      "|                 240|4.79000000|52.31800000|-3.30000000|          Schiphol|2021-08-24|9999-12-31|\n",
      "|                 242|4.92100000|53.24100000|10.80000000|          Vlieland|2021-08-24|9999-12-31|\n",
      "|                 248|5.17400000|52.63400000| 0.80000000|          Wijdenes|2021-08-24|9999-12-31|\n",
      "|                 249|4.97900000|52.64400000|-2.40000000|          Berkhout|2021-08-24|9999-12-31|\n",
      "|                 251|5.34600000|53.39200000| 0.70000000|Hoorn Terschelling|2021-08-24|9999-12-31|\n",
      "|                 257|4.60300000|52.50600000| 8.50000000|      Wijk aan Zee|2021-08-24|9999-12-31|\n",
      "|                 258|5.40100000|52.64900000| 7.30000000|       Houtribdijk|2021-08-24|9999-12-31|\n",
      "|                 260|5.18000000|52.10000000| 1.90000000|           De Bilt|2021-08-24|9999-12-31|\n",
      "|                 265|5.27400000|52.13000000|13.90000000|       Soesterberg|2021-08-24|9999-12-31|\n",
      "|                 267|5.38400000|52.89800000|-1.30000000|          Stavoren|2021-08-24|9999-12-31|\n",
      "|                 269|5.52000000|52.45800000|-3.70000000|          Lelystad|2021-08-24|9999-12-31|\n",
      "|                 270|5.75200000|53.22400000| 1.20000000|        Leeuwarden|2021-08-24|9999-12-31|\n",
      "|                 273|5.88800000|52.70300000|-3.30000000|         Marknesse|2021-08-24|9999-12-31|\n",
      "|                 275|5.87300000|52.05600000|48.20000000|            Deelen|2021-08-24|9999-12-31|\n",
      "|                 277|6.20000000|53.41300000| 2.90000000|        Lauwersoog|2021-08-24|9999-12-31|\n",
      "+--------------------+----------+-----------+-----------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"bronze_weather_stations\")\n",
    "silver_df = spark.sql(\"\"\"\n",
    "                select weather_station_code\n",
    "                     , cast(longitude as DECIMAL(11,8)) as longitude\n",
    "                     , cast(latitude as DECIMAL(11,8)) as latitude\n",
    "                     , cast(altitude as DECIMAL(11,8)) as altitude\n",
    "                     , weather_station\n",
    "                     , to_date(load_datetime) as valid_from\n",
    "                     , coalesce(lead(to_date(load_datetime)) OVER (PARTITION BY weather_station_code ORDER BY load_datetime), to_date('9999-12-31')) as valid_to\n",
    "                from bronze_weather_stations\n",
    "                order by weather_station_code, valid_from\n",
    "            \"\"\").coalesce(1)\n",
    "silver_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15d3d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table if not exists silver_knmi_daggegevens\n",
    "using delta \n",
    "location 's3a://silver-knmi/daggegevens'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8004e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|weather_station_code|   string|       |\n",
      "|                date|     date|       |\n",
      "|vector_mean_wind_...|      int|       |\n",
      "|vector_mean_winds...|   double|       |\n",
      "|daily_mean_windsp...|   double|       |\n",
      "|max_windspeed_in_...|   double|       |\n",
      "|max_windspeed_hou...|   string|       |\n",
      "|min_windspeed_in_...|   double|       |\n",
      "|min_windspeed_hou...|   string|       |\n",
      "|max_windgust_in_m...|   double|       |\n",
      "|max_windgust_hour...|   string|       |\n",
      "|daily_mean_temper...|   double|       |\n",
      "| minimum_temperature|   double|       |\n",
      "|minimum_temperatu...|   string|       |\n",
      "| maximum_temperature|   double|       |\n",
      "|maximum_temperatu...|   string|       |\n",
      "|max_temp_at_10cm_...|   double|       |\n",
      "|max_temp_at_10cm_...|   string|       |\n",
      "|sunshine_duration...|   double|       |\n",
      "|percentage_of_max...|   string|       |\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE silver_knmi_daggegevens\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a658fbe",
   "metadata": {},
   "source": [
    "### Write to the silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d3fd578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging the data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'silver_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5043/2359187502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msilver_knmi_daggegevens_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       .merge(\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msilver_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \"\"\"\n\u001b[1;32m     12\u001b[0m         \u001b[0mcurrent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweather_station_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweather_station_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'silver_df' is not defined"
     ]
    }
   ],
   "source": [
    "if not DeltaTable.isDeltaTable(spark, 's3a://silver-knmi/weather_stations'):\n",
    "    print(\"Not a delta table, write the full df\")\n",
    "    silver_df.write.format(\"delta\").save('s3a://silver-knmi/weather_stations')\n",
    "else:\n",
    "    print(\"Merging the data\")\n",
    "    silver_knmi_daggegevens_table = DeltaTable.forPath(spark, 's3a://silver-knmi/weather_stations')\n",
    "    \n",
    "    silver_knmi_daggegevens_table.alias(\"current_data\") \\\n",
    "      .merge(\n",
    "        silver_df.alias(\"new_data\"),\n",
    "        \"\"\"\n",
    "        current_data.weather_station_code = new_data.weather_station_code\n",
    "        and\n",
    "        current_data.date = new_data.date\n",
    "        \"\"\").whenNotMatchedInsertAll().execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
