{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3312b3",
   "metadata": {},
   "source": [
    "# Initiate spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6b75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/22 21:32:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Spark minIO Test\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://192.168.86.192:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.getenv('MINIO_ROOT_USER'))\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.getenv('MINIO_ROOT_PASSWORD'))\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.driver.memory\", \"8g\")\n",
    "    .set(\"spark.executor.memory\", \"8g\")\n",
    "    .set(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    ")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "#sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc).builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdc2b4",
   "metadata": {},
   "source": [
    "### Determine the last load dat to silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e451d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/22 21:00:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 5:====>                                                    (1 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-20 07:55:59.056056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "last_load_datetime = spark.read.format('delta').load('s3a://silver-knmi/daggegevens').select(max(\"load_datetime\")).collect()[0][\"max(load_datetime)\"]\n",
    "print(last_load_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439ffd2",
   "metadata": {},
   "source": [
    "### Read data from the bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe41bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|YYYYMMDD|\n",
      "+--------+\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('delta').load('s3a://bronze-knmi/daggegevens').filter(col(\"load_datetime\") > last_load_datetime)\n",
    "df.select(\"YYYYMMDD\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bce63",
   "metadata": {},
   "source": [
    "### Transform and rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4e7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, expr\n",
    "silver_df = df.select(col('STN').alias('weather_station_code'),\n",
    "                      to_date(col('YYYYMMDD'), 'yyyyMMdd').alias('date'),\n",
    "                      col('DDVEC').cast(\"integer\").alias(\"vector_mean_wind_direction_in_degrees\"),\n",
    "                      expr('FHVEC / 10').alias(\"vector_mean_windspeed_in_meters_per_second\"),\n",
    "                      expr('FG / 10').alias(\"daily_mean_windspeed_in_meters_per_second\"),\n",
    "                      expr('FHX / 10').alias(\"max_windspeed_in_meters_per_second\"),\n",
    "                      col('FHXH').alias(\"max_windspeed_hour_of_day\"),\n",
    "                      expr('FHN / 10').alias(\"min_windspeed_in_meters_per_second\"),\n",
    "                      expr('FHNH').alias(\"min_windspeed_hour_of_day\"),\n",
    "                      expr('FXX / 10').alias(\"max_windgust_in_meters_per_second\"),\n",
    "                      col('FXXH').alias(\"max_windgust_hour_of_day\"),\n",
    "                      expr('TG / 10').alias(\"daily_mean_temperature\"),\n",
    "                      expr('TN / 10').alias(\"minimum_temperature\"),\n",
    "                      col('TNH').alias(\"minimum_temperature_hour_of_day\"),\n",
    "                      expr('TX / 10').alias(\"maximum_temperature\"),\n",
    "                      col('TXH').alias(\"maximum_temperature_hour_of_day\"),\n",
    "                      expr('T10N / 10').alias(\"max_temp_at_10cm_above_ground\"),\n",
    "                      col('T10NH').alias(\"max_temp_at_10cm_above_ground_hour_of_day\"),\n",
    "                      expr('SQ / 10').alias(\"sunshine_duration_in_hours\"),\n",
    "                      expr('SP').alias(\"percentage_of_max_sumshine_duration\"),\n",
    "                      expr('Q').alias(\"global_radiation_in_J_per_cm2\"),\n",
    "                      expr('DR / 10').alias(\"precipitation_duration_in_hours\"),\n",
    "                      expr('RH / 10').alias(\"daily_precipitaion_amount_in_mm\"),\n",
    "                      expr('RHX / 10').alias(\"max_hourly_precipitaion_amount_in_mm\"),\n",
    "                      col('RHXH').alias(\"max_hourly_precipitaion_hour_of_day\"),\n",
    "                      expr('PG / 10').alias(\"mean_sealevel_pressure_in_hPa\"),\n",
    "                      expr('PX / 10').alias(\"max_sealevel_pressure_in_hPa\"),\n",
    "                      col('PXH').alias(\"max_sealevel_pressure_hour_of_day\"),\n",
    "                      expr('PN / 10').alias(\"min_sealevel_pressure_in_hPa\"),\n",
    "                      col('PNH').alias(\"min_sealevel_pressure_hour_of_day\"),\n",
    "                      expr('VVN * 100').alias(\"minimum_visibility_in_m\"),\n",
    "                      col('VVNH').alias(\"minimum_visibility_hour_of_day\"),\n",
    "                      expr('VVX * 100 + 100').alias(\"maximum_visibility_in_m\"),\n",
    "                      col('VVXH').alias(\"maximum_visibility_hour_of_day\"),\n",
    "                      expr('NG').alias(\"mean_daily_cloud_cover_in_octants\"),\n",
    "                      expr('UG').alias(\"mean_humidity\"),\n",
    "                      expr('UX').alias(\"maximim_humidity\"),\n",
    "                      col('UXH').alias(\"maximum_humidity_hour_of_day\"),\n",
    "                      expr('UN / 10').alias(\"minimum_humidity\"),\n",
    "                      col('UNH').alias(\"minimum_humidity_hour_of_day\"),\n",
    "                      expr('EV24 / 10').alias(\"potential_evapotranspiration_in_mm\"),\n",
    "                      col(\"load_datetime\")\n",
    "            ).coalesce(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15d3d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/22 21:36:04 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 1:=====================================================>   (47 + 3) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists silver_knmi_daggegevens\n",
    "using delta \n",
    "location 's3a://silver-knmi/daggegevens'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "263f804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|silver_knmi_dagge...|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show tables\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a658fbe",
   "metadata": {},
   "source": [
    "### Write to the silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3fd578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====>                                              (144 + 16) / 1377]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3297/1804133038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msilver_knmi_daggegevens_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeltaTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3a://silver-knmi/daggegevens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msilver_knmi_daggegevens_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       .merge(\n\u001b[1;32m     11\u001b[0m         \u001b[0msilver_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/delta/tables.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeltaMergeBuilder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[0musage\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getMatchedBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not DeltaTable.isDeltaTable(spark, 's3a://silver-knmi/daggegevens'):\n",
    "    print(\"Not a delta table, write the full df\")\n",
    "    silver_df.write.format(\"delta\").save('s3a://silver-knmi/daggegevens')\n",
    "else:\n",
    "    print(\"Merging the data\")\n",
    "    silver_knmi_daggegevens_table = DeltaTable.forPath(spark, 's3a://silver-knmi/daggegevens')\n",
    "    \n",
    "    silver_knmi_daggegevens_table.alias(\"current_data\") \\\n",
    "      .merge(\n",
    "        silver_df.alias(\"new_data\"),\n",
    "        \"\"\"\n",
    "        current_data.weather_station_code = new_data.weather_station_code\n",
    "        and\n",
    "        current_data.date = new_data.date\n",
    "        \"\"\").whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f21bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:========================>                           (656 + 16) / 1377]\r"
     ]
    }
   ],
   "source": [
    "testdf = spark.read.format('delta').load('s3a://silver-knmi/daggegevens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e62783ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdf.filter(\"date = '2021-08-16'\").select(col(\"date\"), col(\"weather_station_code\")).show()\n",
    "testdf.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select data, weather_s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
