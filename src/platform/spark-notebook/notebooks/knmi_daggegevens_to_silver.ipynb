{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3312b3",
   "metadata": {},
   "source": [
    "# Initiate spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed6b75e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark minIO Test, master=local[*]) created by __init__ at /tmp/ipykernel_5043/3819819390.py:21 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5043/3819819390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.catalog.spark_catalog\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#sqlContext = SQLContext(sc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark minIO Test, master=local[*]) created by __init__ at /tmp/ipykernel_5043/3819819390.py:21 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Spark minIO Test\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://192.168.86.192:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.getenv('MINIO_ROOT_USER'))\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.getenv('MINIO_ROOT_PASSWORD'))\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.driver.memory\", \"8g\")\n",
    "    .set(\"spark.executor.memory\", \"8g\")\n",
    "    .set(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    ")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "#sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc).builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdc2b4",
   "metadata": {},
   "source": [
    "### Determine the last load dat to silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e451d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:====>                                                    (1 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-20 07:55:59.056056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:======================>                                   (5 + 8) / 13]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "last_load_datetime = spark.read.format('delta').load('s3a://silver-knmi/daggegevens').select(max(\"load_datetime\")).collect()[0][\"max(load_datetime)\"]\n",
    "print(last_load_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439ffd2",
   "metadata": {},
   "source": [
    "### Read data from the bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe41bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|YYYYMMDD|\n",
      "+--------+\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('delta').load('s3a://bronze-knmi/daggegevens').filter(col(\"load_datetime\") > last_load_datetime)\n",
    "df.select(\"YYYYMMDD\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bce63",
   "metadata": {},
   "source": [
    "### Transform and rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4e7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, expr\n",
    "silver_df = df.select(col('STN').alias('weather_station_code'),\n",
    "                      to_date(col('YYYYMMDD'), 'yyyyMMdd').alias('date'),\n",
    "                      col('DDVEC').cast(\"integer\").alias(\"vector_mean_wind_direction_in_degrees\"),\n",
    "                      expr('FHVEC / 10').alias(\"vector_mean_windspeed_in_meters_per_second\"),\n",
    "                      expr('FG / 10').alias(\"daily_mean_windspeed_in_meters_per_second\"),\n",
    "                      expr('FHX / 10').alias(\"max_windspeed_in_meters_per_second\"),\n",
    "                      col('FHXH').alias(\"max_windspeed_hour_of_day\"),\n",
    "                      expr('FHN / 10').alias(\"min_windspeed_in_meters_per_second\"),\n",
    "                      expr('FHNH').alias(\"min_windspeed_hour_of_day\"),\n",
    "                      expr('FXX / 10').alias(\"max_windgust_in_meters_per_second\"),\n",
    "                      col('FXXH').alias(\"max_windgust_hour_of_day\"),\n",
    "                      expr('TG / 10').alias(\"daily_mean_temperature\"),\n",
    "                      expr('TN / 10').alias(\"minimum_temperature\"),\n",
    "                      col('TNH').alias(\"minimum_temperature_hour_of_day\"),\n",
    "                      expr('TX / 10').alias(\"maximum_temperature\"),\n",
    "                      col('TXH').alias(\"maximum_temperature_hour_of_day\"),\n",
    "                      expr('T10N / 10').alias(\"max_temp_at_10cm_above_ground\"),\n",
    "                      col('T10NH').alias(\"max_temp_at_10cm_above_ground_hour_of_day\"),\n",
    "                      expr('SQ / 10').alias(\"sunshine_duration_in_hours\"),\n",
    "                      expr('SP').alias(\"percentage_of_max_sumshine_duration\"),\n",
    "                      expr('Q').alias(\"global_radiation_in_J_per_cm2\"),\n",
    "                      expr('DR / 10').alias(\"precipitation_duration_in_hours\"),\n",
    "                      expr('RH / 10').alias(\"daily_precipitaion_amount_in_mm\"),\n",
    "                      expr('RHX / 10').alias(\"max_hourly_precipitaion_amount_in_mm\"),\n",
    "                      col('RHXH').alias(\"max_hourly_precipitaion_hour_of_day\"),\n",
    "                      expr('PG / 10').alias(\"mean_sealevel_pressure_in_hPa\"),\n",
    "                      expr('PX / 10').alias(\"max_sealevel_pressure_in_hPa\"),\n",
    "                      col('PXH').alias(\"max_sealevel_pressure_hour_of_day\"),\n",
    "                      expr('PN / 10').alias(\"min_sealevel_pressure_in_hPa\"),\n",
    "                      col('PNH').alias(\"min_sealevel_pressure_hour_of_day\"),\n",
    "                      expr('VVN * 100').alias(\"minimum_visibility_in_m\"),\n",
    "                      col('VVNH').alias(\"minimum_visibility_hour_of_day\"),\n",
    "                      expr('VVX * 100 + 100').alias(\"maximum_visibility_in_m\"),\n",
    "                      col('VVXH').alias(\"maximum_visibility_hour_of_day\"),\n",
    "                      expr('NG').alias(\"mean_daily_cloud_cover_in_octants\"),\n",
    "                      expr('UG').alias(\"mean_humidity\"),\n",
    "                      expr('UX').alias(\"maximim_humidity\"),\n",
    "                      col('UXH').alias(\"maximum_humidity_hour_of_day\"),\n",
    "                      expr('UN / 10').alias(\"minimum_humidity\"),\n",
    "                      col('UNH').alias(\"minimum_humidity_hour_of_day\"),\n",
    "                      expr('EV24 / 10').alias(\"potential_evapotranspiration_in_mm\"),\n",
    "                      col(\"load_datetime\")\n",
    "            ).coalesce(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c15d3d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists silver_knmi_daggegevens\n",
    "using delta \n",
    "location 's3a://silver-knmi/daggegevens'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8004e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|weather_station_code|   string|       |\n",
      "|                date|     date|       |\n",
      "|vector_mean_wind_...|      int|       |\n",
      "|vector_mean_winds...|   double|       |\n",
      "|daily_mean_windsp...|   double|       |\n",
      "|max_windspeed_in_...|   double|       |\n",
      "|max_windspeed_hou...|   string|       |\n",
      "|min_windspeed_in_...|   double|       |\n",
      "|min_windspeed_hou...|   string|       |\n",
      "|max_windgust_in_m...|   double|       |\n",
      "|max_windgust_hour...|   string|       |\n",
      "|daily_mean_temper...|   double|       |\n",
      "| minimum_temperature|   double|       |\n",
      "|minimum_temperatu...|   string|       |\n",
      "| maximum_temperature|   double|       |\n",
      "|maximum_temperatu...|   string|       |\n",
      "|max_temp_at_10cm_...|   double|       |\n",
      "|max_temp_at_10cm_...|   string|       |\n",
      "|sunshine_duration...|   double|       |\n",
      "|percentage_of_max...|   string|       |\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE silver_knmi_daggegevens\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a658fbe",
   "metadata": {},
   "source": [
    "### Write to the silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d3fd578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging the data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'silver_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5043/2359187502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msilver_knmi_daggegevens_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       .merge(\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msilver_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \"\"\"\n\u001b[1;32m     12\u001b[0m         \u001b[0mcurrent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweather_station_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweather_station_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'silver_df' is not defined"
     ]
    }
   ],
   "source": [
    "if not DeltaTable.isDeltaTable(spark, 's3a://silver-knmi/daggegevens'):\n",
    "    print(\"Not a delta table, write the full df\")\n",
    "    silver_df.write.format(\"delta\").save('s3a://silver-knmi/daggegevens')\n",
    "else:\n",
    "    print(\"Merging the data\")\n",
    "    silver_knmi_daggegevens_table = DeltaTable.forPath(spark, 's3a://silver-knmi/daggegevens')\n",
    "    \n",
    "    silver_knmi_daggegevens_table.alias(\"current_data\") \\\n",
    "      .merge(\n",
    "        silver_df.alias(\"new_data\"),\n",
    "        \"\"\"\n",
    "        current_data.weather_station_code = new_data.weather_station_code\n",
    "        and\n",
    "        current_data.date = new_data.date\n",
    "        \"\"\").whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f21bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:========================>                           (656 + 16) / 1377]\r"
     ]
    }
   ],
   "source": [
    "testdf = spark.read.format('delta').load('s3a://silver-knmi/daggegevens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e62783ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdf.filter(\"date = '2021-08-16'\").select(col(\"date\"), col(\"weather_station_code\")).show()\n",
    "testdf.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select data, weather_s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
